{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/praymesh/CNN_CIFAR10/blob/main/cifar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "elbX2lJg6lol"
      },
      "outputs": [],
      "source": [
        "import  torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f3SistCl6lom",
        "outputId": "e284f1f2-2032-43e6-c0c6-f5d6cfe86738",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GSJcxZFn6lon"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DGT3Gzem6lon",
        "outputId": "ce6666fe-71c3-40ba-f25f-9fd8a65de4fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:06<00:00, 28.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "10000\n",
            "782\n",
            "157\n"
          ]
        }
      ],
      "source": [
        "#loading the dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "JpXSox3t6lon",
        "outputId": "11dc9e41-a9fb-4541-d567-fd467bd93ea0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10000, 32, 32, 3)\n",
            "(50000, 32, 32, 3)\n",
            "Dataset CIFAR10\n",
            "    Number of datapoints: 50000\n",
            "    Root location: ./data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "print(test_dataset.data.shape) # 10000 images of 32x32 pixels with 3 channels\n",
        "print(train_dataset.data.shape) # 50000 images\n",
        "print(train_dataset)\n",
        "train_dataset.__getitem__(0)[0].shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DUugMmpy6lon",
        "outputId": "d598123e-0297-4de1-fae9-6a228583b507",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "frog\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def showimg(img):\n",
        "    img = plt.imshow(np.transpose(img, (1, 2, 0))) # convert from tensor to image,pytorch saves in channel first format, convert it to channel last format\n",
        "    plt.show()\n",
        "img, label = train_dataset[0]\n",
        "print(classes[label])\n",
        "showimg(train_dataset[100][0].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "O_kHImbH6lon"
      },
      "outputs": [],
      "source": [
        "class ConvNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5) # 3 input channels, 6 output channels, 5x5 kernel\n",
        "        self.pool = nn.MaxPool2d(2, 2) # 2x2 pool\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input channels, 16 output channels, 5x5 kernel\n",
        "        self.pool =nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(16*5*5, 120) # 16 channels of 5x5 pixels, 120 output channels\n",
        "        self.fc2 = nn.Linear(120, 84) # 120 input channels, 84 output channels\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16*5*5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rwZ4i1Sf6loo"
      },
      "outputs": [],
      "source": [
        "model = ConvNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "QkhOMET06loo",
        "outputId": "c47efcab-403a-47e8-a622-a76260e086d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 3, 32, 32]) torch.Size([64])\n"
          ]
        }
      ],
      "source": [
        "inputs, labels = next(iter(train_loader))\n",
        "print(inputs.shape, labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMYjL4V46loo",
        "outputId": "288379d8-1aaf-47fe-ff93-3cd68e6a6b02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1,   100] loss: 1.056\n",
            "[1,   200] loss: 1.107\n",
            "[1,   300] loss: 1.089\n",
            "[1,   400] loss: 1.092\n",
            "[1,   500] loss: 1.137\n",
            "[1,   600] loss: 1.084\n",
            "[1,   700] loss: 1.098\n",
            "[2,   100] loss: 1.093\n",
            "[2,   200] loss: 1.070\n",
            "[2,   300] loss: 1.089\n",
            "[2,   400] loss: 1.083\n",
            "[2,   500] loss: 1.087\n",
            "[2,   600] loss: 1.104\n",
            "[2,   700] loss: 1.072\n",
            "[3,   100] loss: 1.052\n",
            "[3,   200] loss: 1.080\n",
            "[3,   300] loss: 1.049\n",
            "[3,   400] loss: 1.044\n",
            "[3,   500] loss: 1.055\n",
            "[3,   600] loss: 1.082\n",
            "[3,   700] loss: 1.080\n",
            "[4,   100] loss: 1.031\n",
            "[4,   200] loss: 1.035\n",
            "[4,   300] loss: 1.054\n",
            "[4,   400] loss: 1.040\n",
            "[4,   500] loss: 1.080\n",
            "[4,   600] loss: 1.037\n",
            "[4,   700] loss: 1.058\n",
            "[5,   100] loss: 1.020\n",
            "[5,   200] loss: 1.014\n",
            "[5,   300] loss: 1.062\n",
            "[5,   400] loss: 1.034\n",
            "[5,   500] loss: 1.052\n",
            "[5,   600] loss: 1.050\n",
            "[5,   700] loss: 1.025\n",
            "[6,   100] loss: 1.000\n",
            "[6,   200] loss: 1.020\n",
            "[6,   300] loss: 1.032\n",
            "[6,   400] loss: 1.032\n",
            "[6,   500] loss: 1.007\n",
            "[6,   600] loss: 1.033\n",
            "[6,   700] loss: 1.041\n",
            "[7,   100] loss: 1.015\n",
            "[7,   200] loss: 1.007\n",
            "[7,   300] loss: 1.036\n",
            "[7,   400] loss: 1.009\n",
            "[7,   500] loss: 1.014\n",
            "[7,   600] loss: 0.983\n",
            "[7,   700] loss: 1.001\n",
            "[8,   100] loss: 0.999\n",
            "[8,   200] loss: 0.983\n",
            "[8,   300] loss: 0.987\n",
            "[8,   400] loss: 0.979\n",
            "[8,   500] loss: 0.992\n",
            "[8,   600] loss: 1.003\n",
            "[8,   700] loss: 0.983\n",
            "[9,   100] loss: 0.970\n",
            "[9,   200] loss: 0.951\n",
            "[9,   300] loss: 0.991\n",
            "[9,   400] loss: 0.981\n",
            "[9,   500] loss: 0.996\n",
            "[9,   600] loss: 0.991\n",
            "[9,   700] loss: 0.988\n",
            "[10,   100] loss: 0.947\n",
            "[10,   200] loss: 0.976\n",
            "[10,   300] loss: 0.989\n",
            "[10,   400] loss: 0.983\n",
            "[10,   500] loss: 0.964\n",
            "[10,   600] loss: 0.942\n",
            "[10,   700] loss: 0.977\n",
            "[11,   100] loss: 0.959\n",
            "[11,   200] loss: 0.937\n",
            "[11,   300] loss: 0.957\n",
            "[11,   400] loss: 0.966\n",
            "[11,   500] loss: 0.974\n",
            "[11,   600] loss: 0.967\n",
            "[11,   700] loss: 0.969\n",
            "[12,   100] loss: 0.923\n",
            "[12,   200] loss: 0.960\n",
            "[12,   300] loss: 0.927\n",
            "[12,   400] loss: 0.962\n",
            "[12,   500] loss: 0.937\n",
            "[12,   600] loss: 0.953\n",
            "[12,   700] loss: 0.950\n",
            "[13,   100] loss: 0.935\n",
            "[13,   200] loss: 0.923\n",
            "[13,   300] loss: 0.924\n",
            "[13,   400] loss: 0.929\n",
            "[13,   500] loss: 0.939\n",
            "[13,   600] loss: 0.927\n",
            "[13,   700] loss: 0.935\n",
            "[14,   100] loss: 0.872\n",
            "[14,   200] loss: 0.907\n",
            "[14,   300] loss: 0.925\n",
            "[14,   400] loss: 0.925\n",
            "[14,   500] loss: 0.915\n",
            "[14,   600] loss: 0.932\n",
            "[14,   700] loss: 0.953\n",
            "[15,   100] loss: 0.866\n",
            "[15,   200] loss: 0.932\n",
            "[15,   300] loss: 0.891\n",
            "[15,   400] loss: 0.919\n",
            "[15,   500] loss: 0.908\n",
            "[15,   600] loss: 0.912\n",
            "[15,   700] loss: 0.911\n",
            "[16,   100] loss: 0.878\n",
            "[16,   200] loss: 0.890\n",
            "[16,   300] loss: 0.882\n",
            "[16,   400] loss: 0.899\n",
            "[16,   500] loss: 0.918\n",
            "[16,   600] loss: 0.924\n",
            "[16,   700] loss: 0.906\n",
            "[17,   100] loss: 0.892\n",
            "[17,   200] loss: 0.886\n",
            "[17,   300] loss: 0.887\n",
            "[17,   400] loss: 0.890\n",
            "[17,   500] loss: 0.882\n",
            "[17,   600] loss: 0.881\n",
            "[17,   700] loss: 0.891\n",
            "[18,   100] loss: 0.856\n",
            "[18,   200] loss: 0.854\n",
            "[18,   300] loss: 0.887\n",
            "[18,   400] loss: 0.857\n",
            "[18,   500] loss: 0.866\n",
            "[18,   600] loss: 0.922\n",
            "[18,   700] loss: 0.885\n",
            "[19,   100] loss: 0.859\n",
            "[19,   200] loss: 0.848\n",
            "[19,   300] loss: 0.843\n",
            "[19,   400] loss: 0.864\n",
            "[19,   500] loss: 0.877\n",
            "[19,   600] loss: 0.865\n",
            "[19,   700] loss: 0.878\n",
            "[20,   100] loss: 0.844\n",
            "[20,   200] loss: 0.849\n",
            "[20,   300] loss: 0.857\n",
            "[20,   400] loss: 0.876\n",
            "[20,   500] loss: 0.852\n",
            "[20,   600] loss: 0.856\n",
            "[20,   700] loss: 0.862\n",
            "[21,   100] loss: 0.820\n",
            "[21,   200] loss: 0.859\n",
            "[21,   300] loss: 0.842\n",
            "[21,   400] loss: 0.835\n",
            "[21,   500] loss: 0.836\n",
            "[21,   600] loss: 0.858\n",
            "[21,   700] loss: 0.835\n",
            "[22,   100] loss: 0.817\n",
            "[22,   200] loss: 0.807\n",
            "[22,   300] loss: 0.814\n",
            "[22,   400] loss: 0.847\n",
            "[22,   500] loss: 0.813\n",
            "[22,   600] loss: 0.840\n",
            "[22,   700] loss: 0.877\n",
            "[23,   100] loss: 0.794\n",
            "[23,   200] loss: 0.813\n",
            "[23,   300] loss: 0.835\n",
            "[23,   400] loss: 0.830\n",
            "[23,   500] loss: 0.814\n",
            "[23,   600] loss: 0.839\n",
            "[23,   700] loss: 0.838\n",
            "[24,   100] loss: 0.783\n",
            "[24,   200] loss: 0.790\n",
            "[24,   300] loss: 0.789\n",
            "[24,   400] loss: 0.817\n",
            "[24,   500] loss: 0.857\n",
            "[24,   600] loss: 0.839\n",
            "[24,   700] loss: 0.808\n",
            "[25,   100] loss: 0.771\n",
            "[25,   200] loss: 0.800\n",
            "[25,   300] loss: 0.776\n",
            "[25,   400] loss: 0.792\n",
            "[25,   500] loss: 0.812\n",
            "[25,   600] loss: 0.827\n",
            "[25,   700] loss: 0.832\n",
            "[26,   100] loss: 0.780\n",
            "[26,   200] loss: 0.770\n",
            "[26,   300] loss: 0.802\n",
            "[26,   400] loss: 0.792\n",
            "[26,   500] loss: 0.785\n",
            "[26,   600] loss: 0.795\n",
            "[26,   700] loss: 0.808\n",
            "[27,   100] loss: 0.778\n",
            "[27,   200] loss: 0.757\n",
            "[27,   300] loss: 0.788\n",
            "[27,   400] loss: 0.783\n",
            "[27,   500] loss: 0.795\n",
            "[27,   600] loss: 0.791\n",
            "[27,   700] loss: 0.777\n",
            "[28,   100] loss: 0.748\n",
            "[28,   200] loss: 0.763\n",
            "[28,   300] loss: 0.786\n",
            "[28,   400] loss: 0.760\n",
            "[28,   500] loss: 0.786\n",
            "[28,   600] loss: 0.757\n",
            "[28,   700] loss: 0.780\n",
            "[29,   100] loss: 0.752\n",
            "[29,   200] loss: 0.744\n",
            "[29,   300] loss: 0.758\n",
            "[29,   400] loss: 0.754\n",
            "[29,   500] loss: 0.782\n",
            "[29,   600] loss: 0.759\n",
            "[29,   700] loss: 0.764\n",
            "[30,   100] loss: 0.757\n",
            "[30,   200] loss: 0.731\n",
            "[30,   300] loss: 0.747\n",
            "[30,   400] loss: 0.738\n",
            "[30,   500] loss: 0.733\n",
            "[30,   600] loss: 0.762\n",
            "[30,   700] loss: 0.757\n",
            " Training is complete\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 30\n",
        "for epoch in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs,labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() #This clears the gradients from the previous iteration.\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(' Training is complete')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bhi8r536loo",
        "outputId": "dc9c4397-2b90-4012-fb70-51cff56590a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network: 62.77 %\n",
            "Accuracy of plane: 68.6 %\n",
            "Accuracy of car: 71.9 %\n",
            "Accuracy of bird: 44.6 %\n",
            "Accuracy of cat: 37.7 %\n",
            "Accuracy of deer: 59.1 %\n",
            "Accuracy of dog: 46.1 %\n",
            "Accuracy of frog: 73.5 %\n",
            "Accuracy of horse: 75.8 %\n",
            "Accuracy of ship: 72.2 %\n",
            "Accuracy of truck: 78.2 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        for i in range(labels.size(0)):  # Use actual batch size instead of hardcoded 64\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "acc = 100.0 * n_correct / n_samples\n",
        "print(f'Accuracy of the network: {acc} %')\n",
        "for i in range(10):\n",
        "    acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "    print(f'Accuracy of {classes[i]}: {acc} %')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4vTZpWa6lop"
      },
      "source": [
        "For 30 epochs the overall acuuuracy was **62.77%**\n",
        "\n",
        "# Lets Try to improve the accuracy of this model :\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as transform"
      ],
      "metadata": {
        "id": "0DQUpVtYKfAy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = transform.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transform.ToTensor(),\n",
        "    transform.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n"
      ],
      "metadata": {
        "id": "LB_JhSYdKXRB"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the dataset\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=t)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=t)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "print(len(train_dataset))\n",
        "print(len(test_dataset))\n",
        "print(len(train_loader))\n",
        "print(len(test_loader))"
      ],
      "metadata": {
        "id": "gy0Nx0PCK1GG",
        "outputId": "be18ffa2-0d0c-4c7e-bf60-0b17cfb4773e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50000\n",
            "10000\n",
            "782\n",
            "157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "blrUhUjc6lop",
        "outputId": "0d39f411-4594-4b1b-84c2-b7bfe754f1f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1,   101] loss: 2.326\n",
            "[1,   201] loss: 2.303\n",
            "[1,   301] loss: 2.301\n",
            "[1,   401] loss: 2.300\n",
            "[1,   501] loss: 2.297\n",
            "[1,   601] loss: 2.294\n",
            "[1,   701] loss: 2.290\n",
            "[2,   101] loss: 2.293\n",
            "[2,   201] loss: 2.246\n",
            "[2,   301] loss: 2.213\n",
            "[2,   401] loss: 2.161\n",
            "[2,   501] loss: 2.134\n",
            "[2,   601] loss: 2.084\n",
            "[2,   701] loss: 2.041\n",
            "[3,   101] loss: 2.010\n",
            "[3,   201] loss: 1.975\n",
            "[3,   301] loss: 1.950\n",
            "[3,   401] loss: 1.911\n",
            "[3,   501] loss: 1.906\n",
            "[3,   601] loss: 1.889\n",
            "[3,   701] loss: 1.850\n",
            "[4,   101] loss: 1.836\n",
            "[4,   201] loss: 1.797\n",
            "[4,   301] loss: 1.769\n",
            "[4,   401] loss: 1.739\n",
            "[4,   501] loss: 1.737\n",
            "[4,   601] loss: 1.712\n",
            "[4,   701] loss: 1.696\n",
            "[5,   101] loss: 1.689\n",
            "[5,   201] loss: 1.654\n",
            "[5,   301] loss: 1.646\n",
            "[5,   401] loss: 1.630\n",
            "[5,   501] loss: 1.620\n",
            "[5,   601] loss: 1.595\n",
            "[5,   701] loss: 1.613\n",
            "[6,   101] loss: 1.581\n",
            "[6,   201] loss: 1.568\n",
            "[6,   301] loss: 1.568\n",
            "[6,   401] loss: 1.546\n",
            "[6,   501] loss: 1.555\n",
            "[6,   601] loss: 1.541\n",
            "[6,   701] loss: 1.549\n",
            "[7,   101] loss: 1.515\n",
            "[7,   201] loss: 1.499\n",
            "[7,   301] loss: 1.518\n",
            "[7,   401] loss: 1.507\n",
            "[7,   501] loss: 1.479\n",
            "[7,   601] loss: 1.472\n",
            "[7,   701] loss: 1.499\n",
            "[8,   101] loss: 1.476\n",
            "[8,   201] loss: 1.441\n",
            "[8,   301] loss: 1.460\n",
            "[8,   401] loss: 1.461\n",
            "[8,   501] loss: 1.471\n",
            "[8,   601] loss: 1.457\n",
            "[8,   701] loss: 1.411\n",
            "[9,   101] loss: 1.433\n",
            "[9,   201] loss: 1.404\n",
            "[9,   301] loss: 1.422\n",
            "[9,   401] loss: 1.397\n",
            "[9,   501] loss: 1.417\n",
            "[9,   601] loss: 1.393\n",
            "[9,   701] loss: 1.385\n",
            "[10,   101] loss: 1.414\n",
            "[10,   201] loss: 1.354\n",
            "[10,   301] loss: 1.362\n",
            "[10,   401] loss: 1.369\n",
            "[10,   501] loss: 1.358\n",
            "[10,   601] loss: 1.360\n",
            "[10,   701] loss: 1.346\n",
            "[11,   101] loss: 1.352\n",
            "[11,   201] loss: 1.353\n",
            "[11,   301] loss: 1.344\n",
            "[11,   401] loss: 1.332\n",
            "[11,   501] loss: 1.330\n",
            "[11,   601] loss: 1.279\n",
            "[11,   701] loss: 1.310\n",
            "[12,   101] loss: 1.331\n",
            "[12,   201] loss: 1.292\n",
            "[12,   301] loss: 1.292\n",
            "[12,   401] loss: 1.295\n",
            "[12,   501] loss: 1.293\n",
            "[12,   601] loss: 1.285\n",
            "[12,   701] loss: 1.266\n",
            "[13,   101] loss: 1.295\n",
            "[13,   201] loss: 1.280\n",
            "[13,   301] loss: 1.275\n",
            "[13,   401] loss: 1.259\n",
            "[13,   501] loss: 1.247\n",
            "[13,   601] loss: 1.239\n",
            "[13,   701] loss: 1.246\n",
            "[14,   101] loss: 1.249\n",
            "[14,   201] loss: 1.255\n",
            "[14,   301] loss: 1.206\n",
            "[14,   401] loss: 1.219\n",
            "[14,   501] loss: 1.238\n",
            "[14,   601] loss: 1.209\n",
            "[14,   701] loss: 1.220\n",
            "[15,   101] loss: 1.214\n",
            "[15,   201] loss: 1.205\n",
            "[15,   301] loss: 1.200\n",
            "[15,   401] loss: 1.176\n",
            "[15,   501] loss: 1.226\n",
            "[15,   601] loss: 1.188\n",
            "[15,   701] loss: 1.194\n",
            "[16,   101] loss: 1.200\n",
            "[16,   201] loss: 1.184\n",
            "[16,   301] loss: 1.173\n",
            "[16,   401] loss: 1.166\n",
            "[16,   501] loss: 1.179\n",
            "[16,   601] loss: 1.168\n",
            "[16,   701] loss: 1.171\n",
            "[17,   101] loss: 1.158\n",
            "[17,   201] loss: 1.172\n",
            "[17,   301] loss: 1.166\n",
            "[17,   401] loss: 1.161\n",
            "[17,   501] loss: 1.170\n",
            "[17,   601] loss: 1.144\n",
            "[17,   701] loss: 1.136\n",
            "[18,   101] loss: 1.146\n",
            "[18,   201] loss: 1.136\n",
            "[18,   301] loss: 1.141\n",
            "[18,   401] loss: 1.123\n",
            "[18,   501] loss: 1.131\n",
            "[18,   601] loss: 1.123\n",
            "[18,   701] loss: 1.127\n",
            "[19,   101] loss: 1.139\n",
            "[19,   201] loss: 1.112\n",
            "[19,   301] loss: 1.092\n",
            "[19,   401] loss: 1.148\n",
            "[19,   501] loss: 1.131\n",
            "[19,   601] loss: 1.103\n",
            "[19,   701] loss: 1.127\n",
            "[20,   101] loss: 1.120\n",
            "[20,   201] loss: 1.094\n",
            "[20,   301] loss: 1.099\n",
            "[20,   401] loss: 1.091\n",
            "[20,   501] loss: 1.083\n",
            "[20,   601] loss: 1.110\n",
            "[20,   701] loss: 1.101\n",
            "[21,   101] loss: 1.081\n",
            "[21,   201] loss: 1.070\n",
            "[21,   301] loss: 1.077\n",
            "[21,   401] loss: 1.065\n",
            "[21,   501] loss: 1.085\n",
            "[21,   601] loss: 1.097\n",
            "[21,   701] loss: 1.074\n",
            "[22,   101] loss: 1.083\n",
            "[22,   201] loss: 1.051\n",
            "[22,   301] loss: 1.045\n",
            "[22,   401] loss: 1.084\n",
            "[22,   501] loss: 1.085\n",
            "[22,   601] loss: 1.066\n",
            "[22,   701] loss: 1.049\n",
            "[23,   101] loss: 1.057\n",
            "[23,   201] loss: 1.048\n",
            "[23,   301] loss: 1.060\n",
            "[23,   401] loss: 1.053\n",
            "[23,   501] loss: 1.023\n",
            "[23,   601] loss: 1.046\n",
            "[23,   701] loss: 1.064\n",
            "[24,   101] loss: 1.035\n",
            "[24,   201] loss: 1.049\n",
            "[24,   301] loss: 1.024\n",
            "[24,   401] loss: 1.037\n",
            "[24,   501] loss: 1.061\n",
            "[24,   601] loss: 1.058\n",
            "[24,   701] loss: 1.025\n",
            "[25,   101] loss: 1.006\n",
            "[25,   201] loss: 1.026\n",
            "[25,   301] loss: 1.028\n",
            "[25,   401] loss: 1.034\n",
            "[25,   501] loss: 1.028\n",
            "[25,   601] loss: 1.026\n",
            "[25,   701] loss: 1.027\n",
            "[26,   101] loss: 1.007\n",
            "[26,   201] loss: 0.995\n",
            "[26,   301] loss: 1.021\n",
            "[26,   401] loss: 1.010\n",
            "[26,   501] loss: 0.995\n",
            "[26,   601] loss: 1.007\n",
            "[26,   701] loss: 1.003\n",
            "[27,   101] loss: 1.004\n",
            "[27,   201] loss: 1.003\n",
            "[27,   301] loss: 0.987\n",
            "[27,   401] loss: 1.000\n",
            "[27,   501] loss: 0.986\n",
            "[27,   601] loss: 1.017\n",
            "[27,   701] loss: 1.012\n",
            "[28,   101] loss: 0.993\n",
            "[28,   201] loss: 1.002\n",
            "[28,   301] loss: 1.002\n",
            "[28,   401] loss: 0.985\n",
            "[28,   501] loss: 1.000\n",
            "[28,   601] loss: 0.961\n",
            "[28,   701] loss: 0.973\n",
            "[29,   101] loss: 0.979\n",
            "[29,   201] loss: 0.984\n",
            "[29,   301] loss: 0.952\n",
            "[29,   401] loss: 0.980\n",
            "[29,   501] loss: 0.949\n",
            "[29,   601] loss: 0.980\n",
            "[29,   701] loss: 0.993\n",
            "[30,   101] loss: 0.981\n",
            "[30,   201] loss: 0.962\n",
            "[30,   301] loss: 0.939\n",
            "[30,   401] loss: 0.964\n",
            "[30,   501] loss: 0.963\n",
            "[30,   601] loss: 0.977\n",
            "[30,   701] loss: 0.954\n",
            "[31,   101] loss: 0.962\n",
            "[31,   201] loss: 0.949\n",
            "[31,   301] loss: 0.951\n",
            "[31,   401] loss: 0.972\n",
            "[31,   501] loss: 0.935\n",
            "[31,   601] loss: 0.948\n",
            "[31,   701] loss: 0.943\n",
            "[32,   101] loss: 0.927\n",
            "[32,   201] loss: 0.937\n",
            "[32,   301] loss: 0.956\n",
            "[32,   401] loss: 0.933\n",
            "[32,   501] loss: 0.936\n",
            "[32,   601] loss: 0.941\n",
            "[32,   701] loss: 0.932\n",
            "[33,   101] loss: 0.921\n",
            "[33,   201] loss: 0.927\n",
            "[33,   301] loss: 0.919\n",
            "[33,   401] loss: 0.924\n",
            "[33,   501] loss: 0.920\n",
            "[33,   601] loss: 0.949\n",
            "[33,   701] loss: 0.932\n",
            "[34,   101] loss: 0.913\n",
            "[34,   201] loss: 0.926\n",
            "[34,   301] loss: 0.922\n",
            "[34,   401] loss: 0.915\n",
            "[34,   501] loss: 0.912\n",
            "[34,   601] loss: 0.919\n",
            "[34,   701] loss: 0.938\n",
            "[35,   101] loss: 0.883\n",
            "[35,   201] loss: 0.897\n",
            "[35,   301] loss: 0.924\n",
            "[35,   401] loss: 0.929\n",
            "[35,   501] loss: 0.912\n",
            "[35,   601] loss: 0.900\n",
            "[35,   701] loss: 0.901\n",
            "[36,   101] loss: 0.901\n",
            "[36,   201] loss: 0.897\n",
            "[36,   301] loss: 0.918\n",
            "[36,   401] loss: 0.889\n",
            "[36,   501] loss: 0.897\n",
            "[36,   601] loss: 0.894\n",
            "[36,   701] loss: 0.892\n",
            "[37,   101] loss: 0.917\n",
            "[37,   201] loss: 0.876\n",
            "[37,   301] loss: 0.889\n",
            "[37,   401] loss: 0.905\n",
            "[37,   501] loss: 0.911\n",
            "[37,   601] loss: 0.874\n",
            "[37,   701] loss: 0.887\n",
            "[38,   101] loss: 0.887\n",
            "[38,   201] loss: 0.879\n",
            "[38,   301] loss: 0.873\n",
            "[38,   401] loss: 0.902\n",
            "[38,   501] loss: 0.869\n",
            "[38,   601] loss: 0.893\n",
            "[38,   701] loss: 0.871\n",
            "[39,   101] loss: 0.868\n",
            "[39,   201] loss: 0.863\n",
            "[39,   301] loss: 0.874\n",
            "[39,   401] loss: 0.853\n",
            "[39,   501] loss: 0.882\n",
            "[39,   601] loss: 0.865\n",
            "[39,   701] loss: 0.884\n",
            "[40,   101] loss: 0.857\n",
            "[40,   201] loss: 0.863\n",
            "[40,   301] loss: 0.859\n",
            "[40,   401] loss: 0.861\n",
            "[40,   501] loss: 0.865\n",
            "[40,   601] loss: 0.874\n",
            "[40,   701] loss: 0.862\n",
            "[41,   101] loss: 0.846\n",
            "[41,   201] loss: 0.868\n",
            "[41,   301] loss: 0.878\n",
            "[41,   401] loss: 0.854\n",
            "[41,   501] loss: 0.833\n",
            "[41,   601] loss: 0.870\n",
            "[41,   701] loss: 0.865\n",
            "[42,   101] loss: 0.851\n",
            "[42,   201] loss: 0.865\n",
            "[42,   301] loss: 0.847\n",
            "[42,   401] loss: 0.833\n",
            "[42,   501] loss: 0.836\n",
            "[42,   601] loss: 0.842\n",
            "[42,   701] loss: 0.854\n",
            "[43,   101] loss: 0.841\n",
            "[43,   201] loss: 0.825\n",
            "[43,   301] loss: 0.845\n",
            "[43,   401] loss: 0.834\n",
            "[43,   501] loss: 0.828\n",
            "[43,   601] loss: 0.848\n",
            "[43,   701] loss: 0.873\n",
            "[44,   101] loss: 0.850\n",
            "[44,   201] loss: 0.822\n",
            "[44,   301] loss: 0.811\n",
            "[44,   401] loss: 0.846\n",
            "[44,   501] loss: 0.835\n",
            "[44,   601] loss: 0.820\n",
            "[44,   701] loss: 0.853\n",
            "[45,   101] loss: 0.829\n",
            "[45,   201] loss: 0.824\n",
            "[45,   301] loss: 0.836\n",
            "[45,   401] loss: 0.816\n",
            "[45,   501] loss: 0.825\n",
            "[45,   601] loss: 0.829\n",
            "[45,   701] loss: 0.830\n",
            "[46,   101] loss: 0.818\n",
            "[46,   201] loss: 0.799\n",
            "[46,   301] loss: 0.825\n",
            "[46,   401] loss: 0.834\n",
            "[46,   501] loss: 0.826\n",
            "[46,   601] loss: 0.789\n",
            "[46,   701] loss: 0.821\n",
            "[47,   101] loss: 0.814\n",
            "[47,   201] loss: 0.798\n",
            "[47,   301] loss: 0.807\n",
            "[47,   401] loss: 0.799\n",
            "[47,   501] loss: 0.821\n",
            "[47,   601] loss: 0.806\n",
            "[47,   701] loss: 0.813\n",
            "[48,   101] loss: 0.801\n",
            "[48,   201] loss: 0.808\n",
            "[48,   301] loss: 0.792\n",
            "[48,   401] loss: 0.803\n",
            "[48,   501] loss: 0.806\n",
            "[48,   601] loss: 0.829\n",
            "[48,   701] loss: 0.825\n",
            "[49,   101] loss: 0.794\n",
            "[49,   201] loss: 0.786\n",
            "[49,   301] loss: 0.795\n",
            "[49,   401] loss: 0.829\n",
            "[49,   501] loss: 0.798\n",
            "[49,   601] loss: 0.803\n",
            "[49,   701] loss: 0.768\n",
            "[50,   101] loss: 0.776\n",
            "[50,   201] loss: 0.803\n",
            "[50,   301] loss: 0.797\n",
            "[50,   401] loss: 0.787\n",
            "[50,   501] loss: 0.788\n",
            "[50,   601] loss: 0.784\n",
            "[50,   701] loss: 0.793\n",
            "[51,   101] loss: 0.793\n",
            "[51,   201] loss: 0.788\n",
            "[51,   301] loss: 0.763\n",
            "[51,   401] loss: 0.790\n",
            "[51,   501] loss: 0.783\n",
            "[51,   601] loss: 0.773\n",
            "[51,   701] loss: 0.798\n",
            "[52,   101] loss: 0.787\n",
            "[52,   201] loss: 0.769\n",
            "[52,   301] loss: 0.764\n",
            "[52,   401] loss: 0.788\n",
            "[52,   501] loss: 0.776\n",
            "[52,   601] loss: 0.789\n",
            "[52,   701] loss: 0.795\n",
            "[53,   101] loss: 0.768\n",
            "[53,   201] loss: 0.755\n",
            "[53,   301] loss: 0.770\n",
            "[53,   401] loss: 0.811\n",
            "[53,   501] loss: 0.762\n",
            "[53,   601] loss: 0.753\n",
            "[53,   701] loss: 0.784\n",
            "[54,   101] loss: 0.773\n",
            "[54,   201] loss: 0.749\n",
            "[54,   301] loss: 0.746\n",
            "[54,   401] loss: 0.763\n",
            "[54,   501] loss: 0.747\n",
            "[54,   601] loss: 0.779\n",
            "[54,   701] loss: 0.785\n",
            "[55,   101] loss: 0.743\n",
            "[55,   201] loss: 0.745\n",
            "[55,   301] loss: 0.755\n",
            "[55,   401] loss: 0.763\n",
            "[55,   501] loss: 0.770\n",
            "[55,   601] loss: 0.751\n",
            "[55,   701] loss: 0.765\n",
            "[56,   101] loss: 0.766\n",
            "[56,   201] loss: 0.752\n",
            "[56,   301] loss: 0.748\n",
            "[56,   401] loss: 0.770\n",
            "[56,   501] loss: 0.749\n",
            "[56,   601] loss: 0.748\n",
            "[56,   701] loss: 0.760\n",
            "[57,   101] loss: 0.730\n",
            "[57,   201] loss: 0.737\n",
            "[57,   301] loss: 0.737\n",
            "[57,   401] loss: 0.754\n",
            "[57,   501] loss: 0.762\n",
            "[57,   601] loss: 0.740\n",
            "[57,   701] loss: 0.759\n",
            "[58,   101] loss: 0.742\n",
            "[58,   201] loss: 0.710\n",
            "[58,   301] loss: 0.734\n",
            "[58,   401] loss: 0.742\n",
            "[58,   501] loss: 0.761\n",
            "[58,   601] loss: 0.745\n",
            "[58,   701] loss: 0.770\n",
            "[59,   101] loss: 0.745\n",
            "[59,   201] loss: 0.745\n",
            "[59,   301] loss: 0.726\n",
            "[59,   401] loss: 0.753\n",
            "[59,   501] loss: 0.734\n",
            "[59,   601] loss: 0.731\n",
            "[59,   701] loss: 0.751\n",
            "[60,   101] loss: 0.727\n",
            "[60,   201] loss: 0.717\n",
            "[60,   301] loss: 0.716\n",
            "[60,   401] loss: 0.738\n",
            "[60,   501] loss: 0.723\n",
            "[60,   601] loss: 0.722\n",
            "[60,   701] loss: 0.752\n",
            "[61,   101] loss: 0.713\n",
            "[61,   201] loss: 0.704\n",
            "[61,   301] loss: 0.735\n",
            "[61,   401] loss: 0.720\n",
            "[61,   501] loss: 0.742\n",
            "[61,   601] loss: 0.715\n",
            "[61,   701] loss: 0.740\n",
            "[62,   101] loss: 0.718\n",
            "[62,   201] loss: 0.707\n",
            "[62,   301] loss: 0.705\n",
            "[62,   401] loss: 0.687\n",
            "[62,   501] loss: 0.734\n",
            "[62,   601] loss: 0.740\n",
            "[62,   701] loss: 0.734\n",
            "[63,   101] loss: 0.699\n",
            "[63,   201] loss: 0.697\n",
            "[63,   301] loss: 0.708\n",
            "[63,   401] loss: 0.705\n",
            "[63,   501] loss: 0.727\n",
            "[63,   601] loss: 0.717\n",
            "[63,   701] loss: 0.705\n",
            "[64,   101] loss: 0.707\n",
            "[64,   201] loss: 0.709\n",
            "[64,   301] loss: 0.707\n",
            "[64,   401] loss: 0.709\n",
            "[64,   501] loss: 0.698\n",
            "[64,   601] loss: 0.710\n",
            "[64,   701] loss: 0.727\n",
            "[65,   101] loss: 0.684\n",
            "[65,   201] loss: 0.699\n",
            "[65,   301] loss: 0.717\n",
            "[65,   401] loss: 0.716\n",
            "[65,   501] loss: 0.724\n",
            "[65,   601] loss: 0.707\n",
            "[65,   701] loss: 0.708\n",
            "[66,   101] loss: 0.700\n",
            "[66,   201] loss: 0.692\n",
            "[66,   301] loss: 0.713\n",
            "[66,   401] loss: 0.681\n",
            "[66,   501] loss: 0.706\n",
            "[66,   601] loss: 0.701\n",
            "[66,   701] loss: 0.690\n",
            "[67,   101] loss: 0.696\n",
            "[67,   201] loss: 0.658\n",
            "[67,   301] loss: 0.706\n",
            "[67,   401] loss: 0.687\n",
            "[67,   501] loss: 0.691\n",
            "[67,   601] loss: 0.719\n",
            "[67,   701] loss: 0.691\n",
            "[68,   101] loss: 0.693\n",
            "[68,   201] loss: 0.708\n",
            "[68,   301] loss: 0.685\n",
            "[68,   401] loss: 0.675\n",
            "[68,   501] loss: 0.687\n",
            "[68,   601] loss: 0.686\n",
            "[68,   701] loss: 0.692\n",
            "[69,   101] loss: 0.677\n",
            "[69,   201] loss: 0.650\n",
            "[69,   301] loss: 0.697\n",
            "[69,   401] loss: 0.686\n",
            "[69,   501] loss: 0.682\n",
            "[69,   601] loss: 0.686\n",
            "[69,   701] loss: 0.697\n",
            "[70,   101] loss: 0.663\n",
            "[70,   201] loss: 0.681\n",
            "[70,   301] loss: 0.682\n",
            "[70,   401] loss: 0.684\n",
            "[70,   501] loss: 0.683\n",
            "[70,   601] loss: 0.679\n",
            "[70,   701] loss: 0.667\n",
            "[71,   101] loss: 0.690\n",
            "[71,   201] loss: 0.669\n",
            "[71,   301] loss: 0.672\n",
            "[71,   401] loss: 0.665\n",
            "[71,   501] loss: 0.667\n",
            "[71,   601] loss: 0.693\n",
            "[71,   701] loss: 0.671\n",
            "[72,   101] loss: 0.684\n",
            "[72,   201] loss: 0.679\n",
            "[72,   301] loss: 0.673\n",
            "[72,   401] loss: 0.676\n",
            "[72,   501] loss: 0.662\n",
            "[72,   601] loss: 0.688\n",
            "[72,   701] loss: 0.663\n",
            "[73,   101] loss: 0.669\n",
            "[73,   201] loss: 0.658\n",
            "[73,   301] loss: 0.651\n",
            "[73,   401] loss: 0.663\n",
            "[73,   501] loss: 0.655\n",
            "[73,   601] loss: 0.694\n",
            "[73,   701] loss: 0.688\n",
            "[74,   101] loss: 0.660\n",
            "[74,   201] loss: 0.635\n",
            "[74,   301] loss: 0.673\n",
            "[74,   401] loss: 0.662\n",
            "[74,   501] loss: 0.660\n",
            "[74,   601] loss: 0.662\n",
            "[74,   701] loss: 0.692\n",
            "[75,   101] loss: 0.655\n",
            "[75,   201] loss: 0.640\n",
            "[75,   301] loss: 0.644\n",
            "[75,   401] loss: 0.643\n",
            "[75,   501] loss: 0.644\n",
            "[75,   601] loss: 0.657\n",
            "[75,   701] loss: 0.669\n",
            "[76,   101] loss: 0.664\n",
            "[76,   201] loss: 0.639\n",
            "[76,   301] loss: 0.646\n",
            "[76,   401] loss: 0.665\n",
            "[76,   501] loss: 0.665\n",
            "[76,   601] loss: 0.664\n",
            "[76,   701] loss: 0.645\n",
            "[77,   101] loss: 0.631\n",
            "[77,   201] loss: 0.645\n",
            "[77,   301] loss: 0.633\n",
            "[77,   401] loss: 0.656\n",
            "[77,   501] loss: 0.668\n",
            "[77,   601] loss: 0.663\n",
            "[77,   701] loss: 0.654\n",
            "[78,   101] loss: 0.643\n",
            "[78,   201] loss: 0.630\n",
            "[78,   301] loss: 0.637\n",
            "[78,   401] loss: 0.642\n",
            "[78,   501] loss: 0.643\n",
            "[78,   601] loss: 0.650\n",
            "[78,   701] loss: 0.642\n",
            "[79,   101] loss: 0.626\n",
            "[79,   201] loss: 0.628\n",
            "[79,   301] loss: 0.611\n",
            "[79,   401] loss: 0.659\n",
            "[79,   501] loss: 0.655\n",
            "[79,   601] loss: 0.679\n",
            "[79,   701] loss: 0.649\n",
            "[80,   101] loss: 0.640\n",
            "[80,   201] loss: 0.620\n",
            "[80,   301] loss: 0.630\n",
            "[80,   401] loss: 0.622\n",
            "[80,   501] loss: 0.625\n",
            "[80,   601] loss: 0.637\n",
            "[80,   701] loss: 0.659\n",
            "[81,   101] loss: 0.649\n",
            "[81,   201] loss: 0.614\n",
            "[81,   301] loss: 0.607\n",
            "[81,   401] loss: 0.639\n",
            "[81,   501] loss: 0.633\n",
            "[81,   601] loss: 0.642\n",
            "[81,   701] loss: 0.655\n",
            "[82,   101] loss: 0.624\n",
            "[82,   201] loss: 0.604\n",
            "[82,   301] loss: 0.610\n",
            "[82,   401] loss: 0.635\n",
            "[82,   501] loss: 0.626\n",
            "[82,   601] loss: 0.644\n",
            "[82,   701] loss: 0.659\n",
            "[83,   101] loss: 0.630\n",
            "[83,   201] loss: 0.616\n",
            "[83,   301] loss: 0.622\n",
            "[83,   401] loss: 0.603\n",
            "[83,   501] loss: 0.615\n",
            "[83,   601] loss: 0.614\n",
            "[83,   701] loss: 0.640\n",
            "[84,   101] loss: 0.604\n",
            "[84,   201] loss: 0.614\n",
            "[84,   301] loss: 0.609\n",
            "[84,   401] loss: 0.615\n",
            "[84,   501] loss: 0.628\n",
            "[84,   601] loss: 0.602\n",
            "[84,   701] loss: 0.619\n",
            "[85,   101] loss: 0.615\n",
            "[85,   201] loss: 0.620\n",
            "[85,   301] loss: 0.618\n",
            "[85,   401] loss: 0.610\n",
            "[85,   501] loss: 0.626\n",
            "[85,   601] loss: 0.637\n",
            "[85,   701] loss: 0.628\n",
            "[86,   101] loss: 0.609\n",
            "[86,   201] loss: 0.583\n",
            "[86,   301] loss: 0.612\n",
            "[86,   401] loss: 0.636\n",
            "[86,   501] loss: 0.640\n",
            "[86,   601] loss: 0.626\n",
            "[86,   701] loss: 0.623\n",
            "[87,   101] loss: 0.588\n",
            "[87,   201] loss: 0.619\n",
            "[87,   301] loss: 0.608\n",
            "[87,   401] loss: 0.588\n",
            "[87,   501] loss: 0.627\n",
            "[87,   601] loss: 0.599\n",
            "[87,   701] loss: 0.621\n",
            "[88,   101] loss: 0.602\n",
            "[88,   201] loss: 0.586\n",
            "[88,   301] loss: 0.608\n",
            "[88,   401] loss: 0.586\n",
            "[88,   501] loss: 0.624\n",
            "[88,   601] loss: 0.625\n",
            "[88,   701] loss: 0.608\n",
            "[89,   101] loss: 0.587\n",
            "[89,   201] loss: 0.607\n",
            "[89,   301] loss: 0.601\n",
            "[89,   401] loss: 0.610\n",
            "[89,   501] loss: 0.591\n",
            "[89,   601] loss: 0.591\n",
            "[89,   701] loss: 0.606\n",
            "[90,   101] loss: 0.583\n",
            "[90,   201] loss: 0.561\n",
            "[90,   301] loss: 0.601\n",
            "[90,   401] loss: 0.597\n",
            "[90,   501] loss: 0.620\n",
            "[90,   601] loss: 0.600\n",
            "[90,   701] loss: 0.605\n",
            "[91,   101] loss: 0.570\n",
            "[91,   201] loss: 0.583\n",
            "[91,   301] loss: 0.597\n",
            "[91,   401] loss: 0.590\n",
            "[91,   501] loss: 0.593\n",
            "[91,   601] loss: 0.599\n",
            "[91,   701] loss: 0.600\n",
            "[92,   101] loss: 0.579\n",
            "[92,   201] loss: 0.578\n",
            "[92,   301] loss: 0.565\n",
            "[92,   401] loss: 0.592\n",
            "[92,   501] loss: 0.593\n",
            "[92,   601] loss: 0.601\n",
            "[92,   701] loss: 0.618\n",
            "[93,   101] loss: 0.570\n",
            "[93,   201] loss: 0.582\n",
            "[93,   301] loss: 0.578\n",
            "[93,   401] loss: 0.592\n",
            "[93,   501] loss: 0.586\n",
            "[93,   601] loss: 0.586\n",
            "[93,   701] loss: 0.588\n",
            "[94,   101] loss: 0.577\n",
            "[94,   201] loss: 0.573\n",
            "[94,   301] loss: 0.599\n",
            "[94,   401] loss: 0.574\n",
            "[94,   501] loss: 0.590\n",
            "[94,   601] loss: 0.582\n",
            "[94,   701] loss: 0.596\n",
            "[95,   101] loss: 0.584\n",
            "[95,   201] loss: 0.584\n",
            "[95,   301] loss: 0.569\n",
            "[95,   401] loss: 0.584\n",
            "[95,   501] loss: 0.571\n",
            "[95,   601] loss: 0.577\n",
            "[95,   701] loss: 0.587\n",
            "[96,   101] loss: 0.580\n",
            "[96,   201] loss: 0.572\n",
            "[96,   301] loss: 0.567\n",
            "[96,   401] loss: 0.573\n",
            "[96,   501] loss: 0.564\n",
            "[96,   601] loss: 0.585\n",
            "[96,   701] loss: 0.602\n",
            "[97,   101] loss: 0.573\n",
            "[97,   201] loss: 0.555\n",
            "[97,   301] loss: 0.583\n",
            "[97,   401] loss: 0.590\n",
            "[97,   501] loss: 0.568\n",
            "[97,   601] loss: 0.567\n",
            "[97,   701] loss: 0.573\n",
            "[98,   101] loss: 0.573\n",
            "[98,   201] loss: 0.576\n",
            "[98,   301] loss: 0.557\n",
            "[98,   401] loss: 0.560\n",
            "[98,   501] loss: 0.583\n",
            "[98,   601] loss: 0.589\n",
            "[98,   701] loss: 0.595\n",
            "[99,   101] loss: 0.556\n",
            "[99,   201] loss: 0.544\n",
            "[99,   301] loss: 0.562\n",
            "[99,   401] loss: 0.558\n",
            "[99,   501] loss: 0.549\n",
            "[99,   601] loss: 0.581\n",
            "[99,   701] loss: 0.587\n",
            "[100,   101] loss: 0.555\n",
            "[100,   201] loss: 0.568\n",
            "[100,   301] loss: 0.548\n",
            "[100,   401] loss: 0.544\n",
            "[100,   501] loss: 0.572\n",
            "[100,   601] loss: 0.564\n",
            "[100,   701] loss: 0.597\n",
            " Training is complete\n"
          ]
        }
      ],
      "source": [
        "n_epochs = 100\n",
        "for epoch in range(n_epochs):\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs,labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad() #This clears the gradients from the previous iteration.\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i>0 and i % 100 == 0:\n",
        "\n",
        "          print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 100))  # Divide by 1000\n",
        "          running_loss = 0.0  # Reset loss\n",
        "\n",
        "\n",
        "print(' Training is complete')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "epoch_losses = []\n",
        "\n",
        "plt.plot(range(1, len(epoch_losses) + 1), loss)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b3Hp3UK9SDfP",
        "outputId": "5b18305d-304f-4879-9bec-d71f545648f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-d2f415a2ed2a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mepoch_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_losses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3827\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3828\u001b[0m ) -> list[Line2D]:\n\u001b[0;32m-> 3829\u001b[0;31m     return gca().plot(\n\u001b[0m\u001b[1;32m   3830\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m         \u001b[0mscalex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1775\u001b[0m         \"\"\"\n\u001b[1;32m   1776\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1777\u001b[0;31m         \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1778\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, axes, data, return_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m             yield from self._plot_args(\n\u001b[0m\u001b[1;32m    298\u001b[0m                 \u001b[0maxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mambiguous_fmt_datakey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/cbook.py\u001b[0m in \u001b[0;36m_check_1d\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;34m\"\"\"Convert scalars to 1D arrays; pass-through arrays as is.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;31m# Unpack in case of e.g. Pandas or xarray object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1351\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpack_to_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1352\u001b[0m     \u001b[0;31m# plot requires `shape` and `ndim`.  If passed an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;31m# object that doesn't provide them, then force to numpy array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/cbook.py\u001b[0m in \u001b[0;36m_unpack_to_numpy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2359\u001b[0m         \u001b[0;31m# https://numpy.org/devdocs/user/basics.interoperability.html#using-arbitrary-objects-in-numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2360\u001b[0m         \u001b[0;31m# therefore, let arrays do better if they can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2361\u001b[0;31m         \u001b[0mxtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2363\u001b[0m         \u001b[0;31m# In case np.asarray method does not return a numpy array in future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S85I1ptaWJD6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}